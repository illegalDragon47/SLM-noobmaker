# SLM Maker Configuration File
# This file contains default configurations for the SLM Maker CLI
# 
# HOW TO USE THIS FILE:
# 1. Copy this file to your project directory
# 2. Modify the values according to your needs
# 3. Use: python slm_maker.py --config config.yaml
# 
# TIP: Keep the original file as a backup and create custom configs for different projects

# Project Information
# These are metadata fields that identify your project
project:
  name: "SLM Maker"                    # Change this to your project name
  version: "1.0.0"                     # Increment this for new versions
  description: "Interactive Small Language Model Creator"  # Describe your project
  author: "panindrapalnati"            # Your name or organization
  license: "MIT"                       # License type (MIT, Apache, GPL, etc.)

# Default Architecture Configurations
# Each architecture defines a different transformer variant
# You can modify these or add new ones
architectures:
  gpt_style:
    name: "GPT-Style Transformer"      # Display name in CLI
    description: "Standard GPT-style transformer with causal attention"  # Description
    base_class: "GPTStyleArchitecture" # Python class name (don't change)
    default_params:
      # CORE PARAMETERS - Adjust these based on your needs:
      n_layer: 6                       # Number of transformer layers (1-100, typical: 4-24)
      n_head: 6                        # Number of attention heads (1-128, must divide n_embd)
      n_embd: 384                      # Embedding dimension (64-8192, typical: 256-1024)
      vocab_size: 50257                # Vocabulary size (1000-1000000, typical: 50000-100000)
      block_size: 128                  # Context window size (32-8192, typical: 128-2048)
      dropout: 0.1                     # Dropout rate (0.0-0.9, typical: 0.1-0.2)
      bias: true                       # Use bias in linear layers (true/false)
    features:                          # List of features (for display only)
      - "Causal self-attention"
      - "Layer normalization"
      - "GELU activation"
      - "Standard positional encoding"

  deepseek_style:
    name: "DeepSeek-Style Transformer"
    description: "DeepSeek-style with RoPE, RMSNorm, and SwiGLU"
    base_class: "DeepSeekStyleArchitecture"
    default_params:
      # Same core parameters as above
      n_layer: 6                       # Adjust based on your compute budget
      n_head: 6                        # Must be a factor of n_embd
      n_embd: 384                      # Higher = more capacity, more memory
      vocab_size: 50257                # Match your tokenizer vocabulary
      block_size: 128                  # Higher = longer sequences, more memory
      dropout: 0.1                     # 0.1 for small models, 0.2 for large models
      # DEEPSEEK-SPECIFIC PARAMETERS:
      use_rope: true                   # Use RoPE positional encoding (true/false)
      use_swiglu: true                 # Use SwiGLU activation (true/false)
      use_attention_bias: false        # Attention bias (true/false)
    features:
      - "RoPE positional encoding"
      - "RMSNorm"
      - "SwiGLU activation"
      - "Grouped-query attention"
      - "DeepSeek optimizations"

  qwen_style:
    name: "Qwen-Style Transformer"
    description: "Qwen-style with enhanced attention mechanisms"
    base_class: "QwenStyleArchitecture"
    default_params:
      # Core parameters (same as above)
      n_layer: 6                       # More layers = deeper model
      n_head: 6                        # More heads = better attention
      n_embd: 384                      # Higher = more parameters
      vocab_size: 50257                # Vocabulary size
      block_size: 128                  # Context length
      dropout: 0.1                     # Regularization
      # QWEN-SPECIFIC PARAMETERS:
      use_rope: true                   # RoPE encoding
      use_swiglu: true                 # SwiGLU activation
      use_attention_bias: false        # No attention bias
      attention_dropout: 0.1           # Attention dropout (0.0-0.9)
      embedding_dropout: 0.1           # Embedding dropout (0.0-0.9)
    features:
      - "RoPE positional encoding"
      - "RMSNorm"
      - "SwiGLU activation"
      - "Enhanced attention mechanisms"
      - "Attention dropout"
      - "Embedding dropout"

  eleutherai_style:
    name: "EleutherAI-Style Transformer"
    description: "EleutherAI-style with research optimizations"
    base_class: "EleutherAIStyleArchitecture"
    default_params:
      # Core parameters
      n_layer: 6                       # Transformer depth
      n_head: 6                        # Attention heads
      n_embd: 384                      # Hidden dimension
      vocab_size: 50257                # Vocabulary
      block_size: 128                  # Context window
      dropout: 0.1                     # Dropout rate
      # ELEUTHERAI-SPECIFIC PARAMETERS:
      use_rope: false                  # Standard positional encoding
      use_swiglu: false                # Standard GELU activation
      use_attention_bias: true         # Use attention bias
      weight_tying: true               # Tie input/output embeddings
    features:
      - "GPT-style architecture"
      - "EleutherAI optimizations"
      - "Weight tying"
      - "Standard transformer blocks"

  reasoning:
    name: "Reasoning-Focused Transformer"
    description: "Generic reasoning framework with chain-of-thought"
    base_class: "ReasoningArchitecture"
    default_params:
      # Core parameters
      n_layer: 6                       # Base transformer layers
      n_head: 6                        # Attention heads
      n_embd: 384                      # Hidden dimension
      vocab_size: 50257                # Vocabulary size
      block_size: 128                  # Context length
      dropout: 0.1                     # Dropout rate
      # REASONING-SPECIFIC PARAMETERS:
      use_chain_of_thought: true       # Enable CoT reasoning (true/false)
      use_step_by_step: true           # Step-by-step processing (true/false)
      reasoning_layers: 2              # Additional reasoning layers (1-10)
      reasoning_gates: true            # Use reasoning gates (true/false)
      step_embeddings: true            # Use step embeddings (true/false)
    features:
      - "Generic reasoning framework"
      - "Enhanced reasoning attention"
      - "Chain-of-thought module"
      - "Step-by-step reasoning"
      - "Reasoning gates and classifiers"

# Default Training Configuration
# These are preset training configurations you can choose from
# You can modify these or create new ones
training:
  default:
    name: "Default Training Config"    # Configuration name
    description: "Standard training configuration for most models"  # Description
    # LEARNING RATE SETTINGS:
    learning_rate: 1e-4                # Learning rate (1e-5 to 1e-3, typical: 1e-4)
    # BATCH AND ITERATION SETTINGS:
    batch_size: 32                     # Batch size (8-128, adjust based on memory)
    max_iters: 20000                   # Total training iterations (1000-100000)
    warmup_steps: 1000                 # Linear warmup steps (100-5000)
    eval_iters: 500                    # Evaluation frequency (100-2000)
    gradient_accumulation_steps: 32    # Gradient accumulation (1-128)
    # OPTIMIZER SETTINGS:
    optimizer: "adamw"                 # Optimizer: "adamw", "adam", "sgd"
    weight_decay: 0.1                  # Weight decay (0.01-0.2)
    beta1: 0.9                         # Adam beta1 (0.8-0.99)
    beta2: 0.95                        # Adam beta2 (0.9-0.999)
    gradient_clip: 1.0                 # Gradient clipping (0.1-10.0)
    # ADVANCED SETTINGS:
    mixed_precision: true              # Use mixed precision (true/false)
    save_checkpoint_every: 1000        # Save frequency (100-10000)
    eval_every: 500                    # Evaluation frequency (100-5000)
    log_every: 100                     # Logging frequency (10-1000)

  fast:
    name: "Fast Training Config"
    description: "Faster training with higher learning rate"
    # HIGHER LEARNING RATE FOR FASTER CONVERGENCE:
    learning_rate: 1e-3                # Higher LR = faster but potentially unstable
    batch_size: 64                     # Larger batch = more stable gradients
    max_iters: 10000                   # Fewer iterations for quick results
    warmup_steps: 500                  # Shorter warmup
    eval_iters: 250                    # More frequent evaluation
    gradient_accumulation_steps: 16    # Fewer accumulation steps
    optimizer: "adamw"
    weight_decay: 0.05                 # Lower weight decay for faster training
    beta1: 0.9
    beta2: 0.95
    gradient_clip: 1.0
    mixed_precision: true              # Enable for speed
    save_checkpoint_every: 500         # Save more frequently
    eval_every: 250                    # Evaluate more often
    log_every: 50                      # Log more frequently

  stable:
    name: "Stable Training Config"
    description: "Stable training with lower learning rate"
    # LOWER LEARNING RATE FOR STABILITY:
    learning_rate: 5e-5                # Lower LR = more stable but slower
    batch_size: 16                     # Smaller batch = more stable
    max_iters: 50000                   # More iterations for convergence
    warmup_steps: 2000                 # Longer warmup
    eval_iters: 1000                   # Less frequent evaluation
    gradient_accumulation_steps: 64    # More accumulation steps
    optimizer: "adamw"
    weight_decay: 0.2                  # Higher weight decay for regularization
    beta1: 0.9
    beta2: 0.95
    gradient_clip: 0.5                 # Tighter gradient clipping
    mixed_precision: false             # Disable for stability
    save_checkpoint_every: 2000        # Save less frequently
    eval_every: 1000                   # Evaluate less often
    log_every: 200                     # Log less frequently

# Data Management Configuration
# Configure how your training data is handled
data:
  # SUPPORTED FILE FORMATS:
  supported_formats:
    - ".jsonl"                         # JSON Lines (recommended)
    - ".json"                          # JSON array
    - ".txt"                           # Plain text (one sample per line)
  
  # DEFAULT PREPROCESSING SETTINGS:
  default_preprocessing:
    max_length: 512                    # Maximum sequence length (64-4096)
    truncation: true                   # Truncate long sequences (true/false)
    padding: true                      # Pad short sequences (true/false)
    return_tensors: "pt"               # Return PyTorch tensors ("pt", "np", "tf")
  
  # DIRECTORY STRUCTURE:
  # Change these paths if you want different directory names
  directory_structure:
    raw: "data/raw"                    # Where to place your datasets
    processed: "data/processed"        # Where processed data goes
    examples: "data/examples"          # Example datasets
    templates: "data/templates"        # Data format templates
  
  # FORMAT DETECTION RULES:
  # These define how the system detects your data format
  format_detection:
    instruction_tuning:
      required_fields: ["instruction", "output"]    # Must have these fields
      optional_fields: ["input"]                    # Can have these fields
    chat_format:
      required_fields: ["messages"]                 # Must have messages array
      message_fields: ["role", "content"]          # Each message must have these
    reasoning_format:
      required_fields: ["question", "reasoning", "answer"]  # Reasoning fields
    simple_text:
      required_fields: ["text"]                     # Must have text field
      optional_fields: ["metadata"]                 # Can have metadata

# GPU Management Configuration
# Configure how the system uses your hardware
gpu:
  # DEVICE PRIORITY (order of preference):
  device_priority:
    - "cuda"                           # NVIDIA GPUs (best for training)
    - "mps"                            # Apple Silicon (M1/M2)
    - "vulkan"                         # AMD/Intel GPUs
    - "cpu"                            # CPU fallback
  
  # CUDA SETTINGS (NVIDIA GPUs):
  cuda:
    mixed_precision: true              # Use bfloat16/float16 (true/false)
    gradient_checkpointing: false      # Save memory, slower training (true/false)
    memory_fraction: 0.9               # Use 90% of GPU memory (0.1-1.0)
  
  # MPS SETTINGS (Apple Silicon):
  mps:
    mixed_precision: true              # Enable mixed precision
    gradient_checkpointing: false      # Disable for speed
  
  # VULKAN SETTINGS (AMD/Intel GPUs):
  vulkan:
    mixed_precision: false             # Vulkan has limited mixed precision support
    gradient_checkpointing: true       # Enable to save memory
  
  # CPU SETTINGS:
  cpu:
    mixed_precision: false             # No mixed precision on CPU
    gradient_checkpointing: true       # Enable to save memory
    num_threads: 4                     # Number of CPU threads (1-32)
  
  # FALLBACK STRATEGIES:
  # What to try if primary device fails
  fallback_strategies:
    cuda_fallback: ["vulkan", "cpu"]   # CUDA → Vulkan → CPU
    mps_fallback: ["cpu"]              # MPS → CPU
    vulkan_fallback: ["cpu"]           # Vulkan → CPU
    cpu_fallback: []                   # CPU has no fallback

# Export Configuration
# Configure how models are exported to different formats
export:
  formats:
    # GGUF FORMAT (for llama.cpp):
    gguf:
      enabled: true                    # Enable GGUF export (true/false)
      quantization: ["q4_0", "q4_1", "q5_0", "q5_1", "q8_0"]  # Quantization options
      default_quantization: "q4_1"    # Default quantization level
      llama_cpp_compatible: true       # Ensure llama.cpp compatibility
    
    # ONNX FORMAT (cross-platform):
    onnx:
      enabled: true                    # Enable ONNX export
      opset_version: 11                # ONNX opset version (7-17)
      dynamic_axes: true               # Support dynamic input sizes
      optimize: true                   # Apply ONNX optimizations
    
    # TORCHSCRIPT FORMAT (PyTorch):
    torchscript:
      enabled: true                    # Enable TorchScript export
      trace: true                      # Use tracing (true/false)
      script: true                     # Use scripting (true/false)
      optimize: true                   # Apply optimizations
    
    # HUGGINGFACE FORMAT:
    huggingface:
      enabled: true                    # Enable HuggingFace export
      save_tokenizer: true             # Save tokenizer (true/false)
      create_model_card: true          # Create model card (true/false)
      push_to_hub: false               # Push to HuggingFace Hub (true/false)
    
    # PYTORCH FORMAT:
    pytorch:
      enabled: true                    # Enable PyTorch export
      save_state_dict: true            # Save state dict (true/false)
      save_full_model: true            # Save full model (true/false)
      include_optimizer: false         # Include optimizer state (true/false)
  
  # EXPORT SETTINGS:
  output_directory: "exports"          # Where to save exported models
  model_card_template: "templates/model_card.md"  # Model card template
  requirements_file: "requirements.txt" # Requirements file to include

# Logging Configuration
# Configure logging levels and output
logging:
  level: "INFO"                        # Overall log level: DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"  # Log format
  handlers:
    console:
      level: "INFO"                    # Console log level
      format: "%(levelname)s - %(message)s"  # Console format
    file:
      level: "DEBUG"                   # File log level (more detailed)
      filename: "logs/slm_maker.log"   # Log file path
      max_bytes: 10485760              # Max file size (10MB)
      backup_count: 5                  # Number of backup files
  
  # LOGGER-SPECIFIC LEVELS:
  loggers:
    slm_maker: "INFO"                  # Main CLI logger
    gptarch: "INFO"                    # Architecture logger
    dataset_manager: "INFO"            # Dataset manager logger
    training: "INFO"                   # Training logger
    utils: "INFO"                      # Utilities logger

# CLI Configuration
# Configure the command-line interface appearance and behavior
cli:
  # THEME COLORS (change these for different looks):
  theme:
    primary_color: "blue"              # Main color (blue, green, red, yellow, etc.)
    secondary_color: "green"           # Secondary color
    accent_color: "yellow"             # Accent color
    error_color: "red"                 # Error messages
    warning_color: "yellow"            # Warning messages
    success_color: "green"             # Success messages
  
  # DISPLAY SETTINGS:
  display:
    show_progress_bars: true           # Show progress bars (true/false)
    show_tables: true                  # Show data tables (true/false)
    show_panels: true                  # Show information panels (true/false)
    max_table_rows: 50                 # Max rows in tables (10-1000)
  
  # INTERACTION SETTINGS:
  interaction:
    confirm_actions: true              # Ask for confirmation (true/false)
    auto_save: true                   # Auto-save configurations (true/false)
    backup_configs: true              # Create backup configs (true/false)
    max_history: 100                  # Command history size (10-1000)

# Development Configuration
# Settings for developers and debugging
development:
  debug_mode: false                    # Enable debug mode (true/false)
  verbose_output: false                # Show verbose output (true/false)
  profile_performance: false           # Profile code performance (true/false)
  save_intermediate_results: false     # Save intermediate results (true/false)
  
  # TESTING SETTINGS:
  testing:
    run_tests: false                   # Run tests automatically (true/false)
    test_architectures: false          # Test architecture creation (true/false)
    test_training: false               # Test training loop (true/false)
    test_export: false                 # Test export functions (true/false)
  
  # DOCUMENTATION SETTINGS:
  documentation:
    generate_docs: false               # Auto-generate documentation (true/false)
    include_examples: true             # Include examples in docs (true/false)
    include_api_reference: true        # Include API reference (true/false)

# Security Configuration
# Security and safety settings
security:
  allow_remote_execution: false        # Allow remote code execution (true/false)
  allow_file_overwrite: false          # Allow overwriting existing files (true/false)
  validate_inputs: true                # Validate all inputs (true/false)
  sanitize_outputs: true               # Sanitize outputs (true/false)
  
  # API KEYS (add your keys here):
  api_keys:
    huggingface_hub: ""                # HuggingFace Hub token
    wandb: ""                          # Weights & Biases API key
    openai: ""                         # OpenAI API key
  
  # FILE PERMISSIONS (Unix-style):
  file_permissions:
    config_files: "600"                # Config files: owner read/write only
    model_files: "644"                 # Model files: owner read/write, others read
    log_files: "644"                   # Log files: owner read/write, others read

# Performance Configuration
# Performance and optimization settings
performance:
  # MEMORY SETTINGS:
  memory:
    max_memory_usage: "8GB"            # Maximum memory usage (1GB, 2GB, 4GB, 8GB, 16GB, 32GB)
    enable_gradient_checkpointing: false  # Save memory, slower training (true/false)
    enable_mixed_precision: true       # Use mixed precision (true/false)
  
  # COMPUTATION SETTINGS:
  computation:
    num_workers: 4                     # Number of data loading workers (1-16)
    pin_memory: true                   # Pin memory for faster GPU transfer (true/false)
    prefetch_factor: 2                 # Data prefetch factor (1-4)
  
  # CACHING SETTINGS:
  caching:
    enable_cache: true                 # Enable caching (true/false)
    cache_size: "1GB"                  # Cache size (256MB, 512MB, 1GB, 2GB, 4GB)
    cache_ttl: 3600                    # Cache time-to-live in seconds (1800-7200)

# Default Values for Quick Start
# These are the default settings when you first run SLM Maker
quick_start:
  architecture: "gpt_style"            # Default architecture (gpt_style, deepseek_style, etc.)
  training_config: "default"           # Default training config (default, fast, stable)
  dataset_format: "jsonl"              # Default dataset format (jsonl, json, txt)
  export_format: "gguf"                # Default export format (gguf, onnx, torchscript, etc.)
  device: "auto"                       # Default device (auto, cuda, mps, vulkan, cpu)
  
  # SAMPLE DATA GENERATION:
  sample_data:
    create_examples: true              # Create example datasets (true/false)
    include_templates: true            # Include data format templates (true/false)
    generate_sample_configs: true      # Generate sample configurations (true/false)

# =============================================================================
# CUSTOMIZATION GUIDE:
# =============================================================================
# 
# 1. ARCHITECTURE PARAMETERS:
#    - n_layer: More layers = deeper model, more parameters, more memory
#    - n_head: Must divide n_embd evenly (e.g., n_embd=384, n_head=6, 8, 12, 16, 24, 32, 48, 64, 96, 128, 192, 384)
#    - n_embd: Higher = more capacity, more parameters, more memory
#    - vocab_size: Match your tokenizer vocabulary size
#    - block_size: Higher = longer sequences, more memory usage
#    - dropout: 0.1 for small models, 0.2 for large models
#
# 2. TRAINING PARAMETERS:
#    - learning_rate: Start with 1e-4, increase to 1e-3 for faster training, decrease to 5e-5 for stability
#    - batch_size: Start with 32, increase if you have more memory, decrease if you get OOM errors
#    - max_iters: More iterations = longer training, potentially better results
#    - warmup_steps: 10% of max_iters is a good rule of thumb
#
# 3. MEMORY OPTIMIZATION:
#    - Enable gradient_checkpointing if you get OOM errors
#    - Reduce batch_size if you run out of memory
#    - Use smaller n_embd or n_layer for memory-constrained systems
#
# 4. PERFORMANCE TUNING:
#    - Enable mixed_precision for faster training (if supported)
#    - Increase num_workers for faster data loading
#    - Use appropriate device priority for your hardware
#
# 5. EXPORT SETTINGS:
#    - GGUF: Best for local inference with llama.cpp
#    - ONNX: Best for cross-platform deployment
#    - TorchScript: Best for PyTorch mobile/C++ deployment
#    - HuggingFace: Best for sharing and fine-tuning
#
# =============================================================================
# EXAMPLE CUSTOMIZATIONS:
# =============================================================================
#
# For a small model (fits in 4GB VRAM):
#   n_layer: 4, n_head: 4, n_embd: 256, batch_size: 16
#
# For a medium model (fits in 8GB VRAM):
#   n_layer: 6, n_head: 6, n_embd: 384, batch_size: 32
#
# For a large model (fits in 16GB+ VRAM):
#   n_layer: 8, n_head: 8, n_embd: 512, batch_size: 64
#
# For fast experimentation:
#   Use "fast" training config, smaller models, fewer iterations
#
# For production training:
#   Use "stable" training config, larger models, more iterations
#
# =============================================================================
